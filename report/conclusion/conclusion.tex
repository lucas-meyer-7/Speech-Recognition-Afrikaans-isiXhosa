\graphicspath{{conclusion/fig/}}

\chapter{Conclusion} \label{chap:conclusion}
We investigated whether the use of additional speech data from Dutch and isiZulu can help develop ASR systems for Afrikaans and isiXhosa.
We presented two training strategies. The first strategy involves training only on the target language, and the second strategy 
involves training on a related language first and then on the target language. Each model is trained by fine-tuning the XLS-R model for ASR.
We train several models for Afrikaans and isiXhosa using the two strategies, and we evaluate the performance of each model on our validation and test data.
Our experimental results show that the use of additional data from Dutch and isiXhosa slightly improves the performance
of our Afrikaans and isiXhosa models, while requiring more computational resources to train our models.
One major limitation of our work is the generalization ability of our models. This leads us to what we believe should be addressed in future work.

\section{Future work}
The Afrikaans and isiXhosa datasets were used for our experiments are very small (approximately $7.5$ hours of speech data each).
We recommend for future work that more labeled speech data for Afrikaans and isiXhosa should be collected and used for training and evaluating the ASR models.
This is challenging because both Afrikaans and isiXhosa are currently considered under-resourced languages.

Additionally, we believe that, given enough computational resources, performing wav2vec 2.0 pre-training on a large dataset of unlabeled speech data may improve the accuracy of our Afrikaans and isiXhosa models.
We believe that there are two ways this can be done. 
The first method involves training wav2vec 2.0 from scratch using randomly initialized weights, which would be computationally expensive.
The second method involves training a wav2vec 2.0 model that is initialized with the same weights of the XLS-R model, 
which is basically fine-tuning XLS-R on unlabeled speech data using the wav2vec 2.0 objective function (\ref{par:obj}).
We also recommend that different amounts of data for each language should be used in different experiments to see whether cross-lingual
representations result in better accuracy (after fine-tuning) compared to mono-lingual representations.