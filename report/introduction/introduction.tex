\graphicspath{{introduction/fig/}}

\chapter{Introduction} \label{chap:introduction}
The task of automatic speech recognition (ASR) is to map a given speech recording to the text of the spoken words.
The approach for performing ASR has remained the same for several years, which involves two main steps.
In the first step the speech recording (audio data) is transformed into a feature representation known as speech features,
and in the second step the speech features are mapped to a sequence of characters that represent the text.
Traditionally, supervised learning techniques such as hidden Markov models have been used to perform ASR.
However, self-supervised learning techniques have become popular within the field of ASR and other speech related tasks.
Self-supervised learning allows us to learn feature representations using unlabeled data, which is particularly useful in low-resource settings.
One of these techniques is wav2vec 2.0, which is machine learning model used to learn speech features.
Wav2vec 2.0 can be fine-tuned to variety of speech-related tasks such as ASR, automatic speech translation, and speech classification.
The issue is that training wav2vec 2.0 to learn speech features from unlabeled data is computationally expensive. 
Therefore, in this study we focus on fine-tuning pre-trained wav2vec 2.0 models for ASR for Afrikaans and isiXhosa. 

We use the XLS-R model as our pre-trained model, which is a large-scale wav2vec 2.0 model trained on $128$ different languages.
Our Afrikaans and isiXhosa datasets are created by combining recordings from three different speech corpora to ensure speaker diversity.
We compare two fine-tuning strategies: \emph{basic} fine-tuning and \emph{sequential} fine-tuning.
Basic fine-tuning involves fine-tuning on one language, which is the typical strategy used for fine-tuning XLS-R.
Sequential fine-tuning involves fine-tuning on two languages sequentially, specifically two related languages.
Our assumption is that fine-tuning on a related language before fine-tuning on the target language may improve the accuracy of our ASR models.
We use Dutch for our Afrikaans sequential fine-tuning experiments and we use isiZulu for our isiXhosa sequential fine-tuning experiments.

We create several models using both fine-tuning strategies, and all of the models are evaluated on our test sets using the word error rate (WER) metric.
We found that the sequential fine-tuning approach resulted in more accurate Afrikaans and isiXhosa models overall, 
and our most accurate models were fine-tuned using the sequential fine-tuning strategy.
However, the generalization ability of our best models are still limited, and this is most likely due to the limited size of our datasets.
We believe that in future work, pre-training on a large dataset of unlabeled speech recordings may improve the generalization ability of our models.

The paper is organized as follows. We first present the background of ASR and our explored tasks in Chapter \ref{chap:background}. 
We discuss our datasets and provide an outline of our experimental setup in Chapter \ref{chap:experimental_setup}.
Our experimental results are summarized and discussed in Chapter \ref{chap:results}.
Finally, we conclude our study in Chapter \ref{chap:conclusion}.