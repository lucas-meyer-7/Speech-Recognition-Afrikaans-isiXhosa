{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Dg_HXRyHC-D6",
        "outputId": "46e592d9-1e6c-4ec8-f043-72e8bbaf9ecf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets (from -r requirements.txt (line 1))\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate (from -r requirements.txt (line 2))\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface_hub (from -r requirements.txt (line 3))\n",
            "  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.5.3)\n",
            "Requirement already satisfied: Requests in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (2.31.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (4.66.1)\n",
            "Collecting transformers (from -r requirements.txt (line 10))\n",
            "  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (1.11.2)\n",
            "Collecting accelerate (from -r requirements.txt (line 12))\n",
            "  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (2.3.7)\n",
            "Collecting jiwer (from -r requirements.txt (line 14))\n",
            "  Downloading jiwer-3.0.3-py3-none-any.whl (21 kB)\n",
            "Collecting omegaconf (from -r requirements.txt (line 15))\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (2.0.4)\n",
            "Collecting pyctcdecode (from -r requirements.txt (line 17))\n",
            "  Downloading pyctcdecode-0.5.0-py2.py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 1)) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets->-r requirements.txt (line 1))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets->-r requirements.txt (line 1))\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->-r requirements.txt (line 1))\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 1)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 1)) (3.8.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 1)) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 1)) (6.0.1)\n",
            "Collecting responses<0.19 (from evaluate->-r requirements.txt (line 2))\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r requirements.txt (line 3)) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r requirements.txt (line 3)) (4.5.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 4)) (3.0.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 4)) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 4)) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 4)) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 4)) (0.56.4)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 4)) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 4)) (1.7.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 4)) (0.3.6)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 4)) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 4)) (1.0.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 6)) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from Requests->-r requirements.txt (line 7)) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from Requests->-r requirements.txt (line 7)) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from Requests->-r requirements.txt (line 7)) (2023.7.22)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 8)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 8)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 8)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 8)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 8)) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 8)) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 10)) (2023.6.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 10))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 10))\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 12)) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug->-r requirements.txt (line 13)) (2.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer->-r requirements.txt (line 14)) (8.1.7)\n",
            "Collecting rapidfuzz<4,>=3 (from jiwer->-r requirements.txt (line 14))\n",
            "  Downloading rapidfuzz-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from omegaconf->-r requirements.txt (line 15))\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygtrie<3.0,>=2.1 (from pyctcdecode->-r requirements.txt (line 17))\n",
            "  Downloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
            "Collecting hypothesis<7,>=6.14 (from pyctcdecode->-r requirements.txt (line 17))\n",
            "  Downloading hypothesis-6.86.2-py3-none-any.whl (421 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.3/421.3 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis<7,>=6.14->pyctcdecode->-r requirements.txt (line 17)) (2.4.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis<7,>=6.14->pyctcdecode->-r requirements.txt (line 17)) (1.1.3)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->-r requirements.txt (line 4)) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->-r requirements.txt (line 4)) (67.7.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa->-r requirements.txt (line 4)) (3.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->-r requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->-r requirements.txt (line 4)) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa->-r requirements.txt (line 4)) (1.15.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 8)) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->-r requirements.txt (line 4)) (2.21)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=a5da9aa2cad6d311f81eb0d347894d8f1d0c1a47996725cf9397bc2ab8c2eaa2\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: tokenizers, safetensors, pygtrie, antlr4-python3-runtime, xxhash, rapidfuzz, omegaconf, hypothesis, dill, responses, pyctcdecode, multiprocess, jiwer, huggingface_hub, transformers, datasets, evaluate, accelerate\n",
            "Successfully installed accelerate-0.23.0 antlr4-python3-runtime-4.9.3 datasets-2.14.5 dill-0.3.7 evaluate-0.4.0 huggingface_hub-0.17.2 hypothesis-6.86.2 jiwer-3.0.3 multiprocess-0.70.15 omegaconf-2.3.0 pyctcdecode-0.5.0 pygtrie-2.5.0 rapidfuzz-3.3.0 responses-0.18.0 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2 xxhash-3.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip3 install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6mdaEcoyvkN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7a9a0b6-3dc9-44a7-9cfb-434848e2c2de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Union\n",
        "\n",
        "from utils import SR, WRITE_ACCESS_TOKEN, clear_cache, change_pwd\n",
        "change_pwd()\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(WRITE_ACCESS_TOKEN)\n",
        "from accelerate import Accelerator\n",
        "from accelerate.logging import get_logger\n",
        "from datasets import Audio, Dataset, load_dataset\n",
        "from transformers import (\n",
        "    Wav2Vec2Config,\n",
        "    Wav2Vec2Model,\n",
        "    Wav2Vec2FeatureExtractor,\n",
        "    Wav2Vec2CTCTokenizer,\n",
        "    Wav2Vec2Processor,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyDT8Gj0z9FG"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import math\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Union\n",
        "from tqdm import tqdm\n",
        "\n",
        "import datasets\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from datasets import DatasetDict, concatenate_datasets, load_dataset, IterableDatasetDict\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import transformers\n",
        "from accelerate import Accelerator\n",
        "from accelerate.logging import get_logger\n",
        "from huggingface_hub import Repository\n",
        "from transformers import (\n",
        "    SchedulerType,\n",
        "    Wav2Vec2Config,\n",
        "    Wav2Vec2FeatureExtractor,\n",
        "    Wav2Vec2ForPreTraining,\n",
        "    get_scheduler,\n",
        "    set_seed,\n",
        ")\n",
        "from transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices, _sample_negative_indices\n",
        "from transformers.utils import get_full_repo_name\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgigjKfSyvkX"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, files, sep, sr, audio_column_name, duration_column_name, min_duration, max_duration, dataset_name):\n",
        "        self.sep = sep\n",
        "        self.sr = sr\n",
        "        self.min_duration = min_duration\n",
        "        self.max_duration = max_duration\n",
        "        self.audio_column_name = audio_column_name\n",
        "        self.duration_column_name = duration_column_name\n",
        "        self.dataset_name = dataset_name\n",
        "        self.data = self.load_ds(files)\n",
        "\n",
        "    def load_ds(self, all_files):\n",
        "        li = []\n",
        "        for filename in all_files:\n",
        "\n",
        "            df = pd.read_csv(filename, sep=self.sep, engine=\"python\")\n",
        "            li.append(df)\n",
        "        data = pd.concat(li, axis=0, ignore_index=True)\n",
        "\n",
        "        if self.duration_column_name in data.columns:\n",
        "            data = data[data[self.duration_column_name] >= self.min_duration]\n",
        "            print(\"Mean duration: \", data[self.duration_column_name].mean())\n",
        "        return data\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data.iloc[idx]\n",
        "        batch = {}\n",
        "        batch[\"input_values\"] = sf.read(os.path.join(self.dataset_name, item[\"path\"]))[0]\n",
        "\n",
        "        if len(batch[\"input_values\"])//self.sr > self.max_duration:\n",
        "            start = np.random.randint(0, len(batch[\"input_values\"]) - self.max_duration * self.sr)\n",
        "            batch[\"input_values\"] = batch[\"input_values\"][start : start + int(self.max_duration * self.sr)]\n",
        "\n",
        "        return batch\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForWav2Vec2Pretraining:\n",
        "    \"\"\"\n",
        "    Data collator that will dynamically pad the inputs received and prepare masked indices\n",
        "    for self-supervised pretraining.\n",
        "\n",
        "    Args:\n",
        "        model (:class:`~transformers.Wav2Vec2ForPreTraining`):\n",
        "            The Wav2Vec2 model used for pretraining. The data collator needs to have access\n",
        "            to config and ``_get_feat_extract_output_lengths`` function for correct padding.\n",
        "        feature_extractor (:class:`~transformers.Wav2Vec2FeatureExtractor`):\n",
        "            The processor used for proccessing the data.\n",
        "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
        "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
        "            among:\n",
        "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
        "              sequence if provided).\n",
        "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
        "              maximum acceptable input length for the model if that argument is not provided.\n",
        "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
        "              different lengths).\n",
        "        max_length (:obj:`int`, `optional`):\n",
        "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
        "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
        "            If set will pad the sequence to a multiple of the provided value.\n",
        "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
        "            7.5 (Volta).\n",
        "    \"\"\"\n",
        "\n",
        "    model: Wav2Vec2ForPreTraining\n",
        "    feature_extractor: Wav2Vec2FeatureExtractor\n",
        "    padding: Union[bool, str] = \"longest\"\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # reformat list to dict and set to pytorch format\n",
        "        batch = self.feature_extractor.pad(\n",
        "            features,\n",
        "            padding=self.padding,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        device = batch[\"input_values\"].device\n",
        "        batch_size = batch[\"input_values\"].shape[0]\n",
        "\n",
        "        mask_indices_seq_length = self.model._get_feat_extract_output_lengths(batch[\"input_values\"].shape[-1])\n",
        "        # make sure masked sequence length is a Python scalar\n",
        "        mask_indices_seq_length = int(mask_indices_seq_length)\n",
        "\n",
        "        # make sure that no loss is computed on padded inputs\n",
        "        if batch.get(\"attention_mask\") is not None:\n",
        "            # compute real output lengths according to convolution formula\n",
        "            batch[\"sub_attention_mask\"] = self.model._get_feature_vector_attention_mask(\n",
        "                mask_indices_seq_length, batch[\"attention_mask\"]\n",
        "            )\n",
        "\n",
        "        features_shape = (batch_size, mask_indices_seq_length)\n",
        "\n",
        "        # sample randomly masked indices\n",
        "        mask_time_indices = _compute_mask_indices(\n",
        "            features_shape,\n",
        "            self.model.config.mask_time_prob,\n",
        "            self.model.config.mask_time_length,\n",
        "            attention_mask=batch.get(\"sub_attention_mask\")\n",
        "        )\n",
        "        # sample negative indices\n",
        "        sampled_negative_indices = _sample_negative_indices(\n",
        "            features_shape,\n",
        "            self.model.config.num_negatives,\n",
        "            mask_time_indices=mask_time_indices,\n",
        "        )\n",
        "        batch[\"mask_time_indices\"] = torch.tensor(mask_time_indices, dtype=torch.long, device=device)\n",
        "        batch[\"sampled_negative_indices\"] = torch.tensor(sampled_negative_indices, dtype=torch.long, device=device)\n",
        "\n",
        "        return batch\n",
        "\n",
        "\n",
        "def multiply_grads(params, c):\n",
        "    \"\"\"Multiplies grads by a constant *c*.\"\"\"\n",
        "    for p in params:\n",
        "        if p.grad is not None:\n",
        "            if torch.is_tensor(c):\n",
        "                c = c.to(p.grad.device)\n",
        "            p.grad.data.mul_(c)\n",
        "\n",
        "\n",
        "def get_grad_norm(params, scale=1):\n",
        "    \"\"\"Compute grad norm given a gradient scale.\"\"\"\n",
        "    total_norm = 0.0\n",
        "    for p in params:\n",
        "        if p.grad is not None:\n",
        "            param_norm = (p.grad.detach().data / scale).norm(2)\n",
        "            total_norm += param_norm.item() ** 2\n",
        "    total_norm = total_norm**0.5\n",
        "    return total_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1Sd1o7VDTfm",
        "outputId": "3dda2a49-bc53-4e5e-88bb-4a5bd7e8f93f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading nchlt_afr.tar.gz ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.89G/4.89G [26:47<00:00, 3.04MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File nchlt_afr.tar.gz downloaded successfully!\n",
            "\n",
            "Downloading nchlt_xho.tar.gz ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.06G/5.06G [20:21<00:00, 4.14MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File nchlt_xho.tar.gz downloaded successfully!\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 210/210 [04:29<00:00,  1.28s/it]\n",
            "100%|██████████| 209/209 [03:42<00:00,  1.06s/it]\n",
            "100%|██████████| 103142/103142 [00:00<00:00, 107504.91it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-62ce1335ad66>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mload_nchlt_2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_nchlt_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mload_nchlt_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nchlt_afr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/load_nchlt_2.py\u001b[0m in \u001b[0;36mload_nchlt_2\u001b[0;34m(dataset_name, only_af, only_xh, write_audio)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mcsv_entries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdst_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_entry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_entries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'duration'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mcsv_entries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3718\u001b[0m         )\n\u001b[1;32m   3719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3720\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3721\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3722\u001b[0m             \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineterminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         )\n\u001b[0;32m-> 1189\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \"\"\"\n\u001b[1;32m    240\u001b[0m         \u001b[0;31m# apply compression and byte/text conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         with get_handle(\n\u001b[0m\u001b[1;32m    242\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;31m# Only for write methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m         \u001b[0mcheck_parent_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mrf\"Cannot save file into a non-existent directory: '{parent}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'nchlt_afr'"
          ]
        }
      ],
      "source": [
        "from load_nchlt_2 import load_nchlt_2\n",
        "\n",
        "load_nchlt_2(dataset_name=\"nchlt_afr\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXEEB88x0Cau",
        "outputId": "788a98c0-d73d-4c7d-a2df-e421b894da4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-c76e2dd32e51>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Handle the repository creation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_main_process\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mrepo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRepository\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclone_from\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, local_dir, clone_from, repo_type, token, git_user, git_email, revision, skip_lfs_files, client)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclone_from\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclone_from\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_git_repo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mclone_from\u001b[0;34m(self, repo_url, token)\u001b[0m\n\u001b[1;32m    677\u001b[0m                 \u001b[0;31m# Check if the folder is the root of a git repository\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_git_repo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m                     raise EnvironmentError(\n\u001b[0m\u001b[1;32m    680\u001b[0m                         \u001b[0;34m\"Tried to clone a repository in a non-empty folder that isn't\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                         \u001b[0;34mf\" a git repository ('{self.local_dir}'). If you really want to\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Tried to clone a repository in a non-empty folder that isn't a git repository ('/content/wav2vec2-pretrained-model'). If you really want to do this, do it manually:\n cd /content/wav2vec2-pretrained-model && git init && git remote add origin && git pull origin main\n or clone repo to a new folder and move your existing files there afterwards."
          ]
        }
      ],
      "source": [
        "logger = get_logger(__name__)\n",
        "\n",
        "push_to_hub = True\n",
        "user_name = \"lucas-meyer\"\n",
        "repo_name = \"wav2vec2-from-scratch-nchlt_afr\"\n",
        "resume = False\n",
        "\n",
        "train_datasets = [os.path.join(\"nchlt_afr\", \"train.csv\")]\n",
        "val_datasets = [os.path.join(\"nchlt_afr\", \"validation.csv\")]\n",
        "separator = \",\"\n",
        "audio_column_name = \"path\"\n",
        "duration_column_name = \"duration\"\n",
        "min_duration_in_seconds = 0.5\n",
        "max_duration_in_seconds = 25.0\n",
        "\n",
        "per_device_train_batch_size = 16\n",
        "per_device_eval_batch_size = 8\n",
        "gradient_accumulation_steps = 8\n",
        "learning_rate = 0.0001\n",
        "lr_scheduler_type = \"linear\"\n",
        "adam_beta1 = 0.9\n",
        "adam_beta2 = 0.999\n",
        "adam_epsilon = 1e-8\n",
        "weight_decay = 0.01\n",
        "\n",
        "max_gumbel_temperature = 2.0\n",
        "min_gumbel_temperature = 0.5\n",
        "gumbel_temperature_decay = 0.999995\n",
        "\n",
        "num_train_epochs = 20\n",
        "num_warmup_steps = 32000\n",
        "max_train_steps = 200000\n",
        "saving_steps = 100\n",
        "logging_steps = 1\n",
        "\n",
        "# Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n",
        "accelerator = Accelerator(dispatch_batches=False)\n",
        "logger.info(accelerator.state, main_process_only=False)\n",
        "if accelerator.is_local_main_process:\n",
        "    # set up tensorboard if available\n",
        "    writer = SummaryWriter(repo_name + '/logs', max_queue=5, flush_secs=30)\n",
        "\n",
        "# Set the training seed now.\n",
        "set_seed(42)\n",
        "\n",
        "# Handle the repository creation\n",
        "if accelerator.is_main_process:\n",
        "    repo = Repository(repo_name, clone_from=f\"{user_name}/{repo_name}\")\n",
        "    os.makedirs(repo_name, exist_ok=True)\n",
        "    print(\"poes ya\")\n",
        "\n",
        "accelerator.wait_for_everyone()\n",
        "\n",
        "# Download data\n",
        "train_dataset = CustomDataset(\n",
        "    files=train_datasets,\n",
        "    sep=separator,\n",
        "    audio_column_name=audio_column_name,\n",
        "    duration_column_name=duration_column_name,\n",
        "    sr=SR,\n",
        "    min_duration=min_duration_in_seconds,\n",
        "    max_duration=max_duration_in_seconds,\n",
        "    dataset_name=\"nchlt_afr\",\n",
        ")\n",
        "\n",
        "val_dataset = CustomDataset(\n",
        "    files=val_datasets,\n",
        "    sep=separator,\n",
        "    audio_column_name=audio_column_name,\n",
        "    duration_column_name=duration_column_name,\n",
        "    sr=SR,\n",
        "    min_duration=min_duration_in_seconds,\n",
        "    max_duration=max_duration_in_seconds,\n",
        "    dataset_name=\"nchlt_afr\",\n",
        ")\n",
        "\n",
        "# Load feature_extractor\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "# only normalized-inputs-training is supported\n",
        "if not feature_extractor.do_normalize:\n",
        "    raise ValueError(\n",
        "        \"Training is only supported for normalized inputs. Make sure ``feature_extractor.do_normalize == True``\"\n",
        "    )\n",
        "\n",
        "# Load model config\n",
        "config = Wav2Vec2Config()\n",
        "# if not config.do_stable_layer_norm or config.feat_extract_norm != \"layer\":\n",
        "#     raise ValueError(\n",
        "#         \"PreTraining is only supported for ``config.do_stable_layer_norm=True`` and\"\n",
        "#         \" ``config.feat_extract_norm='layer'\"\n",
        "#     )\n",
        "\n",
        "\n",
        "# initialize random model\n",
        "model = Wav2Vec2ForPreTraining(config)\n",
        "# if load_from_pretrained is not None:\n",
        "#     try:\n",
        "#         model = model.from_pretrained(model_name_or_path)\n",
        "#     except:\n",
        "#         print(\"!!!!! Warning: Pretrained model may not exist. Start training from Scratch\")\n",
        "\n",
        "# Activate gradient checkpointing\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# Define data collator, optimizer and scheduler\n",
        "data_collator = DataCollatorForWav2Vec2Pretraining(\n",
        "    model=model, feature_extractor=feature_extractor, pad_to_multiple_of=None\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=per_device_train_batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=16,\n",
        "    pin_memory=True,\n",
        "    prefetch_factor=16\n",
        ")\n",
        "\n",
        "eval_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=per_device_eval_batch_size,\n",
        "    num_workers=16\n",
        ")\n",
        "\n",
        "# Optimizer\n",
        "optimizer =  torch.optim.AdamW(\n",
        "    list(model.parameters()),\n",
        "    lr=learning_rate,\n",
        "    betas=[adam_beta1, adam_beta2],\n",
        "    eps=adam_epsilon,\n",
        ")\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=lr_scheduler_type,\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    num_training_steps=max_train_steps,\n",
        ")\n",
        "\n",
        "# Prepare everything with our `accelerator`.\n",
        "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, eval_dataloader\n",
        ")\n",
        "if resume:\n",
        "    print(\"******Resume checkpoint******\")\n",
        "    accelerator.load_state(repo_name)\n",
        "    checkpoint = torch.load(os.path.join(repo_name, 'latest_checkpoint.pt'),\n",
        "                            map_location=\"cpu\")\n",
        "\n",
        "\n",
        " # Train\n",
        "total_batch_size = per_device_train_batch_size * accelerator.num_processes * gradient_accumulation_steps\n",
        "\n",
        "# Scheduler and math around the number of training steps.\n",
        "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
        "\n",
        "\n",
        "if max_train_steps is None:\n",
        "    max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "\n",
        "\n",
        "# Afterwards we recalculate our number of training epochs\n",
        "num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n",
        "\n",
        "if accelerator.is_main_process:\n",
        "    print(\"Number of training data: \", len(train_dataset))\n",
        "    print(\"total_batch_size: \", total_batch_size)\n",
        "    print(\"num_update_steps_per_epoch: \", num_update_steps_per_epoch)\n",
        "    print(\"num_train_epochs: \", num_train_epochs)\n",
        "\n",
        "# Only show the progress bar once on each machine.\n",
        "completed_steps = checkpoint['completed_steps'] + 1 if resume else 0\n",
        "starting_epoch = checkpoint['epoch'] if resume else 0\n",
        "progress_bar = tqdm(initial = completed_steps, total = max_train_steps, disable=not accelerator.is_local_main_process)\n",
        "\n",
        "print(f\"******STARTING AT EPOCH {starting_epoch} - STEP {completed_steps}******\")\n",
        "\n",
        "\n",
        "for epoch in range(starting_epoch, num_train_epochs):\n",
        "    if accelerator.is_main_process:\n",
        "        print(f\"\\nEpoch {epoch}: \")\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # compute num of losses\n",
        "        num_losses = batch[\"mask_time_indices\"].sum()\n",
        "        sub_attention_mask = batch.pop(\"sub_attention_mask\", None)\n",
        "        sub_attention_mask = (\n",
        "            sub_attention_mask if sub_attention_mask is not None else torch.ones_like(batch[\"mask_time_indices\"])\n",
        "        )\n",
        "        percent_masked = num_losses / sub_attention_mask.sum()\n",
        "\n",
        "        # forward\n",
        "        outputs = model(**batch)\n",
        "\n",
        "        # divide loss by gradient accumulation steps since gradients\n",
        "        # are accumulated for multiple backward passes in PyTorch\n",
        "        loss = outputs.loss / gradient_accumulation_steps\n",
        "        accelerator.backward(loss)\n",
        "\n",
        "        # make sure that `num_losses` is summed for distributed training\n",
        "        # and average gradients over losses of all devices\n",
        "        if accelerator.state.num_processes > 1:\n",
        "            num_losses = accelerator.gather(num_losses).sum()\n",
        "            gradient_multiplier = accelerator.state.num_processes / num_losses\n",
        "            multiply_grads(model.module.parameters(), gradient_multiplier)\n",
        "        else:\n",
        "            multiply_grads(model.parameters(), 1 / num_losses)\n",
        "\n",
        "        # update step\n",
        "        if (step + 1) % gradient_accumulation_steps == 0:\n",
        "\n",
        "            # compute grad norm for monitoring\n",
        "            scale = (\n",
        "                accelerator.scaler._scale.item()\n",
        "                if hasattr(accelerator, \"scaler\") and accelerator.scaler is not None\n",
        "                else 1\n",
        "            )\n",
        "            if accelerator.state.num_processes > 1:\n",
        "                grad_norm = get_grad_norm(model.module.parameters(), scale)\n",
        "            else:\n",
        "                grad_norm = get_grad_norm(model.parameters(), scale)\n",
        "\n",
        "            # update parameters\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if not accelerator.optimizer_step_was_skipped:\n",
        "                lr_scheduler.step()\n",
        "            elif accelerator.is_local_main_process:\n",
        "                progress_bar.write(\n",
        "                    f\"Gradients have overflown - skipping update step... Updating gradient scale to {scale}...\"\n",
        "                )\n",
        "\n",
        "            # update gumbel temperature\n",
        "            gumbel_temperature = max(\n",
        "                max_gumbel_temperature * gumbel_temperature_decay**completed_steps,\n",
        "                min_gumbel_temperature,\n",
        "            )\n",
        "            if hasattr(model, \"module\"):\n",
        "                model.module.set_gumbel_temperature(gumbel_temperature)\n",
        "            else:\n",
        "                model.set_gumbel_temperature(gumbel_temperature)\n",
        "\n",
        "            progress_bar.update(1)\n",
        "            completed_steps += 1\n",
        "\n",
        "        # Log all results\n",
        "        if (step + 1) % (gradient_accumulation_steps * logging_steps) == 0:\n",
        "            loss.detach()\n",
        "            outputs.contrastive_loss.detach()\n",
        "            outputs.diversity_loss.detach()\n",
        "            cosine_sim = torch.cosine_similarity(outputs.projected_states, outputs.projected_quantized_states, dim=-1)\n",
        "            cosine_sim = cosine_sim[batch[\"mask_time_indices\"].to(torch.bool)].mean()\n",
        "\n",
        "            if accelerator.state.num_processes > 1:\n",
        "                loss = accelerator.gather(loss).sum()\n",
        "                outputs.contrastive_loss = accelerator.gather(outputs.contrastive_loss).sum()\n",
        "                outputs.diversity_loss = accelerator.gather(outputs.diversity_loss).sum()\n",
        "                percent_masked = accelerator.gather(percent_masked).sum()\n",
        "                cosine_sim = accelerator.gather(cosine_sim).mean()\n",
        "\n",
        "            train_logs = {\n",
        "                \"step\": torch.tensor((step + 1) // gradient_accumulation_steps, dtype=torch.int32),\n",
        "                \"loss\": (loss * gradient_accumulation_steps) / num_losses,\n",
        "                \"contrast_loss\": outputs.contrastive_loss / num_losses,\n",
        "                \"div_loss\": outputs.diversity_loss / num_losses,\n",
        "                \"%_mask_idx\": percent_masked / accelerator.num_processes,\n",
        "                \"ppl\": outputs.codevector_perplexity,\n",
        "                \"lr\": torch.tensor(lr_scheduler.get_lr()),\n",
        "                \"temp\": torch.tensor(gumbel_temperature),\n",
        "                \"grad_norm\": torch.tensor(grad_norm),\n",
        "                \"cosine_sim\": cosine_sim * 100\n",
        "            }\n",
        "            log_str = \"\"\n",
        "            for k, v in train_logs.items():\n",
        "                log_str += \"| {}: {:.3e}\".format(k, v.item())\n",
        "\n",
        "            if accelerator.is_local_main_process:\n",
        "                progress_bar.write(log_str)\n",
        "                for k, v in train_logs.items():\n",
        "                    writer.add_scalar('TRAIN' + '/' + k, v, completed_steps)\n",
        "\n",
        "\n",
        "        # save model every `saving_steps` steps\n",
        "        if (step + 1) % (gradient_accumulation_steps * saving_steps) == 0:\n",
        "            if (push_to_hub and epoch < num_train_epochs - 1) or repo_name is not None:\n",
        "                accelerator.wait_for_everyone()\n",
        "                unwrapped_model = accelerator.unwrap_model(model)\n",
        "                unwrapped_model.save_pretrained(\n",
        "                        repo_name + f'/saved_model/epoch_{epoch}', is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
        "                    )\n",
        "                if accelerator.is_main_process:\n",
        "                    feature_extractor.save_pretrained(repo_name + f'/saved_model/epoch_{epoch}')\n",
        "                    print(\"****Saving checkpoint*****\")\n",
        "                    state_dict = {\n",
        "                        \"completed_steps\": completed_steps,\n",
        "                        \"epoch\": epoch\n",
        "                    }\n",
        "                    torch.save(state_dict, os.path.join(repo_name, \"latest_checkpoint.pt\"))\n",
        "                accelerator.save_state(repo_name)\n",
        "\n",
        "            if (push_to_hub and epoch < num_train_epochs - 1) and accelerator.is_main_process:\n",
        "                repo.push_to_hub(\n",
        "                    commit_message=f\"Training in progress step {completed_steps}\",\n",
        "                    blocking=False,\n",
        "                    auto_lfs_prune=True,\n",
        "                )\n",
        "\n",
        "        # if completed steps > `max_train_steps` stop\n",
        "        if completed_steps >= max_train_steps:\n",
        "            break\n",
        "\n",
        "    print(\"******END OF EPOCH******\")\n",
        "    # Validate!\n",
        "    model.eval()\n",
        "\n",
        "    # init logs\n",
        "    val_logs = {\n",
        "        \"val_loss\": 0,\n",
        "        \"val_contrastive_loss\": 0,\n",
        "        \"val_diversity_loss\": 0,\n",
        "        \"val_num_losses\": 0,\n",
        "    }\n",
        "    for step, batch in enumerate(eval_dataloader):\n",
        "        with torch.no_grad():\n",
        "            batch.pop(\"sub_attention_mask\", None)\n",
        "            outputs = model(**batch)\n",
        "\n",
        "        val_logs[\"val_loss\"] += outputs.loss\n",
        "        val_logs[\"val_contrastive_loss\"] += outputs.contrastive_loss\n",
        "        val_logs[\"val_diversity_loss\"] += outputs.diversity_loss\n",
        "        val_logs[\"val_num_losses\"] += batch[\"mask_time_indices\"].sum()\n",
        "\n",
        "    # sum over devices in multi-processing\n",
        "    if accelerator.num_processes > 1:\n",
        "        val_logs = {k: accelerator.gather(v).sum() for k, v in val_logs.items()}\n",
        "\n",
        "    val_logs = {k: v / val_logs[\"val_num_losses\"] for k, v in val_logs.items()}\n",
        "\n",
        "    log_str = \"\"\n",
        "    for k, v in val_logs.items():\n",
        "        log_str += \"| {}: {:.3e}\".format(k, v.item())\n",
        "\n",
        "    if accelerator.is_local_main_process:\n",
        "        progress_bar.write(log_str)\n",
        "        for k, v in val_logs.items():\n",
        "            writer.add_scalar('VALIDATION' + '/' + k, v, epoch)\n",
        "\n",
        "    if repo_name is not None:\n",
        "        accelerator.wait_for_everyone()\n",
        "        unwrapped_model = accelerator.unwrap_model(model)\n",
        "        unwrapped_model.save_pretrained(\n",
        "                repo_name + f'/saved_model/epoch_{epoch}', is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
        "            )\n",
        "        if accelerator.is_main_process:\n",
        "            feature_extractor.save_pretrained(repo_name + f'/saved_model/epoch_{epoch}')\n",
        "            print(\"****Saving checkpoint*****\")\n",
        "            state_dict = {\n",
        "                \"completed_steps\": completed_steps,\n",
        "                \"epoch\": epoch\n",
        "            }\n",
        "            torch.save(state_dict, os.path.join(repo_name, \"latest_checkpoint.pt\"))\n",
        "\n",
        "        accelerator.save_state(repo_name)\n",
        "        if accelerator.is_main_process:\n",
        "            if push_to_hub:\n",
        "                repo.push_to_hub(commit_message=\"End of training\", auto_lfs_prune=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}