{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uk5vdD4y88_T"
   },
   "source": [
    "# Fine-tune a pre-trained model using Afrikaans/isiXhosa data\n",
    "### Author: Lucas Meyer\n",
    "\n",
    "Python notebook used to load a pre-trained model from HuggingFace and fine-tune the model using a HuggingFace dataset (or a custom dataset).\n",
    "\n",
    "**Important Note**: that we recommend that you should upload this notebook (along with the three ``load_<>.py`` scripts, ``requirements.txt``, and ``utils.py``) to a Google Colab session. Make sure to use a T4 GPU at least, and if you do not have access to one then create a Google burner account so that you have immediate access. :)\n",
    "\n",
    "**Acknowledgements**: I would like to thank Patrick von Platen, for his super useful [blog-post](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2) on how to fine-tune XLS-R models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIVqS0kd88_V"
   },
   "source": [
    "### 1. Install dependencies\n",
    "\n",
    "Replace \"awe\" with your sudo password if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_gp1SSYRt1i1"
   },
   "outputs": [],
   "source": [
    "# Run with sudo or not\n",
    "sudo_password = False\n",
    "if sudo_password:\n",
    "    !echo \"awe\" > password.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nIOC8eJo88_Y",
    "outputId": "289b55e5-8ca0-4a0d-d2db-30118ebd305b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets==2.14.4 (from -r requirements.txt (line 4))\n",
      "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting evaluate==0.4.0 (from -r requirements.txt (line 5))\n",
      "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface_hub==0.16.4 (from -r requirements.txt (line 6))\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting librosa==0.10.0.post2 (from -r requirements.txt (line 7))\n",
      "  Downloading librosa-0.10.0.post2-py3-none-any.whl (253 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.23.5)\n",
      "Requirement already satisfied: pandas==1.5.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (1.5.3)\n",
      "Collecting pyctcdecode==0.3.0 (from -r requirements.txt (line 10))\n",
      "  Downloading pyctcdecode-0.3.0-py2.py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (2.31.0)\n",
      "Collecting scipy==1.8.0 (from -r requirements.txt (line 12))\n",
      "  Downloading scipy-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch==2.0.1 (from -r requirements.txt (line 13))\n",
      "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm==4.65.0 (from -r requirements.txt (line 14))\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers==4.28.0 (from -r requirements.txt (line 15))\n",
      "  Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate==0.23.0 (from -r requirements.txt (line 21))\n",
      "  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: werkzeug==3.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (3.0.0)\n",
      "Collecting jiwer==3.0.3 (from -r requirements.txt (line 23))\n",
      "  Downloading jiwer-3.0.3-py3-none-any.whl (21 kB)\n",
      "Collecting kenlm==0.2.0 (from -r requirements.txt (line 24))\n",
      "  Downloading kenlm-0.2.0.tar.gz (427 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.4/427.4 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting urllib3==2.0.6 (from -r requirements.txt (line 25))\n",
      "  Downloading urllib3-2.0.6-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.4->-r requirements.txt (line 4)) (9.0.0)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.14.4->-r requirements.txt (line 4))\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.4->-r requirements.txt (line 4)) (3.4.1)\n",
      "Collecting multiprocess (from datasets==2.14.4->-r requirements.txt (line 4))\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.4->-r requirements.txt (line 4)) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.4->-r requirements.txt (line 4)) (3.8.6)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.4->-r requirements.txt (line 4)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.4->-r requirements.txt (line 4)) (6.0.1)\n",
      "Collecting responses<0.19 (from evaluate==0.4.0->-r requirements.txt (line 5))\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.16.4->-r requirements.txt (line 6)) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.16.4->-r requirements.txt (line 6)) (4.5.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.post2->-r requirements.txt (line 7)) (3.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.post2->-r requirements.txt (line 7)) (1.2.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.post2->-r requirements.txt (line 7)) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.post2->-r requirements.txt (line 7)) (4.4.2)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.post2->-r requirements.txt (line 7)) (0.56.4)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.post2->-r requirements.txt (line 7)) (0.12.1)\n",
      "Collecting pooch<1.7,>=1.0 (from librosa==0.10.0.post2->-r requirements.txt (line 7))\n",
      "  Downloading pooch-1.6.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.post2->-r requirements.txt (line 7)) (0.3.7)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.post2->-r requirements.txt (line 7)) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.post2->-r requirements.txt (line 7)) (1.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->-r requirements.txt (line 9)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->-r requirements.txt (line 9)) (2023.3.post1)\n",
      "Collecting pygtrie<3.0,>=2.1 (from pyctcdecode==0.3.0->-r requirements.txt (line 10))\n",
      "  Downloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
      "Collecting hypothesis<7,>=6.14 (from pyctcdecode==0.3.0->-r requirements.txt (line 10))\n",
      "  Downloading hypothesis-6.88.1-py3-none-any.whl (421 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.2/421.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from Requests==2.31.0->-r requirements.txt (line 11)) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from Requests==2.31.0->-r requirements.txt (line 11)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from Requests==2.31.0->-r requirements.txt (line 11)) (2023.7.22)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 13)) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 13)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 13)) (3.1.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->-r requirements.txt (line 13))\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->-r requirements.txt (line 13))\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->-r requirements.txt (line 13))\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->-r requirements.txt (line 13))\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->-r requirements.txt (line 13))\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->-r requirements.txt (line 13))\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->-r requirements.txt (line 13))\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->-r requirements.txt (line 13))\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->-r requirements.txt (line 13))\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->-r requirements.txt (line 13))\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->-r requirements.txt (line 13))\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.0.0 (from torch==2.0.1->-r requirements.txt (line 13))\n",
      "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0->-r requirements.txt (line 15)) (2023.6.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.0->-r requirements.txt (line 15))\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.23.0->-r requirements.txt (line 21)) (5.9.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug==3.0.0->-r requirements.txt (line 22)) (2.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer==3.0.3->-r requirements.txt (line 23)) (8.1.7)\n",
      "Collecting rapidfuzz<4,>=3 (from jiwer==3.0.3->-r requirements.txt (line 23))\n",
      "  Downloading rapidfuzz-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 13)) (67.7.2)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 13)) (0.41.2)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 13)) (3.27.7)\n",
      "Collecting lit (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 13))\n",
      "  Downloading lit-17.0.3.tar.gz (154 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.4->-r requirements.txt (line 4)) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.4->-r requirements.txt (line 4)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.4->-r requirements.txt (line 4)) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.4->-r requirements.txt (line 4)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.4->-r requirements.txt (line 4)) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.4->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis<7,>=6.14->pyctcdecode==0.3.0->-r requirements.txt (line 10)) (2.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis<7,>=6.14->pyctcdecode==0.3.0->-r requirements.txt (line 10)) (1.1.3)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa==0.10.0.post2->-r requirements.txt (line 7)) (0.39.1)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa==0.10.0.post2->-r requirements.txt (line 7)) (1.4.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3->-r requirements.txt (line 9)) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa==0.10.0.post2->-r requirements.txt (line 7)) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa==0.10.0.post2->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->-r requirements.txt (line 13)) (1.3.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa==0.10.0.post2->-r requirements.txt (line 7)) (2.21)\n",
      "Building wheels for collected packages: kenlm, lit\n",
      "  Building wheel for kenlm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for kenlm: filename=kenlm-0.2.0-cp310-cp310-linux_x86_64.whl size=3184464 sha256=9ca6e05f73f03bfed2160f46c0b34e68a18f267d0ac1d9035895cdd7b8d18342\n",
      "  Stored in directory: /root/.cache/pip/wheels/fd/80/e0/18f4148e863fb137bd87e21ee2bf423b81b3ed6989dab95135\n",
      "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for lit: filename=lit-17.0.3-py3-none-any.whl size=93257 sha256=083ae0507c8b8672fb768d09828fe86883903a8c7c5fead03ce2cf036074a00e\n",
      "  Stored in directory: /root/.cache/pip/wheels/ac/b8/42/f6f56aba870f9f3cc895b2e0c970ececaafc7d191217fa10a4\n",
      "Successfully built kenlm lit\n",
      "Installing collected packages: tokenizers, pygtrie, lit, kenlm, urllib3, tqdm, scipy, rapidfuzz, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, hypothesis, dill, pyctcdecode, nvidia-cusolver-cu11, nvidia-cudnn-cu11, multiprocess, jiwer, responses, pooch, huggingface_hub, transformers, librosa, datasets, evaluate, triton, torch, accelerate\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.7\n",
      "    Uninstalling urllib3-2.0.7:\n",
      "      Successfully uninstalled urllib3-2.0.7\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.1\n",
      "    Uninstalling tqdm-4.66.1:\n",
      "      Successfully uninstalled tqdm-4.66.1\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.11.3\n",
      "    Uninstalling scipy-1.11.3:\n",
      "      Successfully uninstalled scipy-1.11.3\n",
      "  Attempting uninstall: pooch\n",
      "    Found existing installation: pooch 1.7.0\n",
      "    Uninstalling pooch-1.7.0:\n",
      "      Successfully uninstalled pooch-1.7.0\n",
      "  Attempting uninstall: librosa\n",
      "    Found existing installation: librosa 0.10.1\n",
      "    Uninstalling librosa-0.10.1:\n",
      "      Successfully uninstalled librosa-0.10.1\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.0+cu118\n",
      "    Uninstalling torch-2.1.0+cu118:\n",
      "      Successfully uninstalled torch-2.1.0+cu118\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "lida 0.0.10 requires fastapi, which is not installed.\n",
      "lida 0.0.10 requires kaleido, which is not installed.\n",
      "lida 0.0.10 requires python-multipart, which is not installed.\n",
      "lida 0.0.10 requires uvicorn, which is not installed.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
      "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
      "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.23.0 datasets-2.14.4 dill-0.3.7 evaluate-0.4.0 huggingface_hub-0.16.4 hypothesis-6.88.1 jiwer-3.0.3 kenlm-0.2.0 librosa-0.10.0.post2 lit-17.0.3 multiprocess-0.70.15 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pooch-1.6.0 pyctcdecode-0.3.0 pygtrie-2.5.0 rapidfuzz-3.4.0 responses-0.18.0 scipy-1.8.0 tokenizers-0.13.3 torch-2.0.1 tqdm-4.65.0 transformers-4.28.0 triton-2.0.0 urllib3-2.0.6\n",
      "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
      "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
      "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [555 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
      "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
      "Get:8 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,305 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
      "Get:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,274 kB]\n",
      "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,009 kB]\n",
      "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,131 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,398 kB]\n",
      "Get:15 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,330 kB]\n",
      "Hit:17 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Get:18 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [28.0 kB]\n",
      "Get:19 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [40.1 kB]\n",
      "Fetched 8,455 kB in 3s (2,503 kB/s)\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "git-lfs is already the newest version (3.0.2-1ubuntu0.2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 19 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "# Install Python dependencies\n",
    "!pip3 install -r requirements.txt\n",
    "\n",
    "# Install GitLFS\n",
    "if sudo_password:\n",
    "    !sudo -S apt-get update < password.txt\n",
    "    !sudo apt-get install git-lfs tree\n",
    "else:\n",
    "    !apt-get update\n",
    "    !apt-get install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzclDGLf88_Z"
   },
   "source": [
    "### 2. Setup experiment\n",
    "\n",
    "**Important Note**: Change ``WRITE_ACCESS_TOKEN`` to your HF token in ``utils.py``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBm5UthH88_Z"
   },
   "source": [
    "### 2.1 Choose dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GEPxlxoTRAP1",
    "outputId": "88de7e08-5b79-4fa6-e06c-a60351f1b11d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter name of dataset (asr_af OR asr_xh OR fleurs_zu): asr_af\n"
     ]
    }
   ],
   "source": [
    "dataset_name = str(input(\"Enter name of dataset (asr_af OR asr_xh OR fleurs_zu): \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQ5mrpUv88_a"
   },
   "source": [
    "### 2.2 Choose pre-trained model and repo names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mzM_S1Z51kCq",
    "outputId": "3b65f919-b34e-4758-b7ac-6e33267036f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo exists: y\n",
      "Sequential fine-tune: y\n",
      "Provide the pull repo please: lucas-meyer/wav2vec2-xls-r-300m-fleurs_zu-run1-seq-asr_af\n"
     ]
    }
   ],
   "source": [
    "repo_exists = True if (input(\"Repo exists: \") == \"y\") else False\n",
    "sequential = True if (input(\"Sequential fine-tune: \") == \"y\") else False\n",
    "\n",
    "if not repo_exists:\n",
    "    if not sequential:\n",
    "        run_number = int(input(\"Provide the run number please: \"))\n",
    "        PUSH_REPO = f\"xls-r-{dataset_name}-run{run_number}\"\n",
    "        PULL_REPO = \"facebook/wav2vec2-xls-r-300m\"\n",
    "    else:\n",
    "        PULL_REPO = input(\"Provide the pull repo please: \")\n",
    "        run_number = int(input(\"Provide the run number please: \"))\n",
    "        first_part = PULL_REPO.split(\"/\")[1]\n",
    "        PUSH_REPO = f\"seq-{first_part}-{dataset_name}-run{run_number}\"\n",
    "else:\n",
    "    PULL_REPO = input(\"Provide the pull repo please: \")\n",
    "    PUSH_REPO = PULL_REPO.split(\"/\")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4lHBpl988_b"
   },
   "source": [
    "### 3. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-XrjksjFtkKR"
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "# My own imports\n",
    "from load_nchlt import load_nchlt\n",
    "from load_fleurs import load_fleurs\n",
    "from load_fleurs_nl import load_fleurs_nl\n",
    "from load_fleurs_zu import load_fleurs_zu\n",
    "from load_high_quality_tts import load_high_quality_tts\n",
    "from utils import (\n",
    "    SR, WRITE_ACCESS_TOKEN,\n",
    "    clear_cache,\n",
    "    remove_special_characters_batch\n",
    ")\n",
    "\n",
    "# HuggingFace imports\n",
    "from datasets import Audio\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from huggingface_hub import Repository\n",
    "from transformers import Wav2Vec2ForCTC\n",
    "from transformers import Wav2Vec2Processor\n",
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEjIkD7n88_b"
   },
   "source": [
    "### 4. ASR model class\n",
    "\n",
    "I made this for my own convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Ei_vwWcQtkKU"
   },
   "outputs": [],
   "source": [
    "class ASR_MODEL:\n",
    "    def __init__(self, repo_name, dataset_name, load_from_hf, push_dataset, push_repo, write_audio):\n",
    "        self.repo_name = repo_name\n",
    "        self.dataset_name = dataset_name\n",
    "        self.load_from_hf = load_from_hf\n",
    "        self.push_dataset = push_dataset\n",
    "        self.push_repo = push_repo\n",
    "        self.write_audio = write_audio\n",
    "        os.makedirs(repo_name, exist_ok=True)\n",
    "\n",
    "        self.train_set = None\n",
    "        self.val_set = None\n",
    "        self.test_set = None\n",
    "        self.tokenizer = None\n",
    "        self.feature_extractor = None\n",
    "        self.processor = None\n",
    "        self.data_collator = None\n",
    "        self.xlsr_model = None\n",
    "        self.trainer = None\n",
    "\n",
    "        if sequential:\n",
    "            self.xlsr_model = Wav2Vec2ForCTC.from_pretrained(PULL_REPO)\n",
    "            self.processor = Wav2Vec2Processor.from_pretrained(PULL_REPO)\n",
    "            self.tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(PULL_REPO)\n",
    "\n",
    "            self.xlsr_model.freeze_feature_encoder()          # Freeze feature exctraction weights\n",
    "            self.xlsr_model.gradient_checkpointing_enable()   # Enable gradient checkpointing\n",
    "\n",
    "    def load_datasets(self):\n",
    "        self.dataset_dir = os.path.join(\"data\", \"speech_data\", self.dataset_name)\n",
    "        if not self.load_from_hf:\n",
    "            if not os.path.exists(self.dataset_dir):\n",
    "                os.makedirs(self.dataset_dir, exist_ok=True)\n",
    "                # Create dataset by combining 3 datasets into an audiofolder\n",
    "                csv_entries = []\n",
    "                if (self.dataset_name == \"asr_af\"):\n",
    "                    csv_entries += load_fleurs(language=\"af\", write_audio=self.write_audio)\n",
    "                    csv_entries += load_high_quality_tts(language=\"af\", write_audio=self.write_audio)\n",
    "                    # csv_entries += load_nchlt(language=\"af\", write_audio=self.write_audio)\n",
    "                elif (self.dataset_name == \"asr_xh\"):\n",
    "                    csv_entries += load_fleurs(language=\"xh\", write_audio=self.write_audio)\n",
    "                    csv_entries += load_high_quality_tts(language=\"xh\", write_audio=self.write_audio)\n",
    "                    csv_entries += load_nchlt(language=\"xh\", write_audio=self.write_audio)\n",
    "                elif (self.dataset_name == \"asr_af_xh\"):\n",
    "                    csv_entries += load_fleurs(language=\"both\", write_audio=self.write_audio)\n",
    "                    csv_entries += load_high_quality_tts(language=\"both\", write_audio=self.write_audio)\n",
    "                    csv_entries += load_nchlt(language=\"both\", write_audio=self.write_audio)\n",
    "                elif (self.dataset_name == \"fleurs_nl\"):\n",
    "                    csv_entries += load_fleurs_nl(write_audio=self.write_audio, plot_durations=True)\n",
    "                elif (self.dataset_name == \"fleurs_zu\"):\n",
    "                    csv_entries += load_fleurs_zu(write_audio=self.write_audio, plot_durations=True)\n",
    "                metadata = pd.DataFrame(csv_entries, columns=['file_name', 'transcription'])\n",
    "                metadata.to_csv(path_or_buf=os.path.join(self.dataset_dir, \"metadata.csv\"), sep=\",\", index=False)\n",
    "\n",
    "                # Load dataset from audiofolder that you created\n",
    "                dataset = load_dataset(\"audiofolder\", data_dir=self.dataset_dir)\n",
    "\n",
    "                # Push dataset to huggingface hub\n",
    "                if self.push_dataset:\n",
    "                    dataset.push_to_hub(f\"lucas-meyer/{self.dataset_name}\")\n",
    "                    # done = False\n",
    "                    # num_restarts = 0\n",
    "                    # while not done:\n",
    "                    #     try:\n",
    "                    #         dataset.push_to_hub(f\"lucas-meyer/{self.dataset_name}\")\n",
    "                    #         done = True\n",
    "                    #     except Exception as e:\n",
    "                    #         num_restarts += 1\n",
    "                    #         print(f\"{str(e)}\")\n",
    "                    #         print(f\"Restarting (num restarts: {num_restarts})\")\n",
    "            else:\n",
    "                # Load dataset from audiofolder that you created\n",
    "                dataset = load_dataset(\"audiofolder\", data_dir=self.dataset_dir)\n",
    "        else:\n",
    "            # Load dataset from huggingface hub\n",
    "            dataset = load_dataset(f\"lucas-meyer/{self.dataset_name}\")\n",
    "\n",
    "        # Downsample audio to SR = 16000 and init train/val/test sets\n",
    "        self.train_set = dataset[\"train\"].cast_column(\"audio\", Audio(sampling_rate=SR)).rename_column(\"transcription\", \"sentence\")\n",
    "        self.val_set = dataset[\"validation\"].cast_column(\"audio\", Audio(sampling_rate=SR)).rename_column(\"transcription\", \"sentence\")\n",
    "        self.test_set = dataset[\"test\"].cast_column(\"audio\", Audio(sampling_rate=SR)).rename_column(\"transcription\", \"sentence\")\n",
    "        self.train_set = self.train_set.map(remove_special_characters_batch)\n",
    "        self.val_set = self.val_set.map(remove_special_characters_batch)\n",
    "        self.test_set = self.test_set.map(remove_special_characters_batch)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def extract_all_chars(self, batch):\n",
    "        all_text = \" \".join(batch[\"sentence\"])\n",
    "        vocab = list(set(all_text))\n",
    "        return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "    def create_tokenizer(self):\n",
    "        vocab_train = self.train_set.map(self.extract_all_chars,\n",
    "                                         batched=True, batch_size=-1,\n",
    "                                         keep_in_memory=True,\n",
    "                                         remove_columns=self.train_set.column_names)\n",
    "\n",
    "        vocab_val = self.val_set.map(self.extract_all_chars,\n",
    "                                     batched=True, batch_size=-1,\n",
    "                                     keep_in_memory=True,\n",
    "                                     remove_columns=self.val_set.column_names)\n",
    "\n",
    "        vocab_test = self.test_set.map(self.extract_all_chars,\n",
    "                                       batched=True, batch_size=-1,\n",
    "                                       keep_in_memory=True,\n",
    "                                       remove_columns=self.test_set.column_names)\n",
    "\n",
    "        # Get list for vocab of train/val/test\n",
    "        vocab_list = list(set(vocab_train[\"vocab\"][0]) |\n",
    "                        set(vocab_test[\"vocab\"][0]) |\n",
    "                        set(vocab_val[\"vocab\"][0]))\n",
    "\n",
    "        # Get dict for vocab of train/val/test\n",
    "        vocab_dict = {v: k for k, v in enumerate(sorted(vocab_list))}\n",
    "        vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "        del vocab_dict[\" \"]\n",
    "        vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "        vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "\n",
    "        # Save vocabulary file\n",
    "        with open(os.path.join(self.repo_name, 'vocab.json'), 'w') as vocab_file:\n",
    "            json.dump(vocab_dict, vocab_file)\n",
    "\n",
    "        self.tokenizer = Wav2Vec2CTCTokenizer(os.path.join(self.repo_name, 'vocab.json'),\n",
    "                                              unk_token=\"[UNK]\",\n",
    "                                              pad_token=\"[PAD]\",\n",
    "                                              bos_token=None,\n",
    "                                              eos_token=None,\n",
    "                                              word_delimiter_token=\"|\")\n",
    "        if self.push_repo:\n",
    "            self.tokenizer.push_to_hub(f\"lucas-meyer/{self.repo_name}\")\n",
    "\n",
    "    def prepare_dataset(self, batch):\n",
    "        audio = batch[\"audio\"]\n",
    "        batch[\"input_values\"] = self.processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "        batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "        batch[\"labels\"] = self.processor(text=batch[\"sentence\"]).input_ids\n",
    "        return batch\n",
    "\n",
    "    def create_processor(self):\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1,\n",
    "                                                     sampling_rate=16000,\n",
    "                                                     padding_value=0.0,\n",
    "                                                     do_normalize=True,\n",
    "                                                     return_attention_mask=True)\n",
    "\n",
    "        self.processor = Wav2Vec2Processor(feature_extractor=self.feature_extractor,\n",
    "                                      tokenizer=self.tokenizer)\n",
    "\n",
    "    def extract_features(self):\n",
    "        self.train_set = self.train_set.map(self.prepare_dataset, remove_columns=self.train_set.column_names)\n",
    "        self.val_set = self.val_set.map(self.prepare_dataset, remove_columns=self.val_set.column_names)\n",
    "        self.test_set = self.test_set.map(self.prepare_dataset, remove_columns=self.test_set.column_names)\n",
    "\n",
    "    def create_data_collator(self):\n",
    "        self.data_collator = DataCollatorCTCWithPadding(processor=self.processor, padding=True)\n",
    "\n",
    "    def download_xlsr(self):\n",
    "        self.xlsr_model = Wav2Vec2ForCTC.from_pretrained(\n",
    "            PULL_REPO,\n",
    "            attention_dropout=0.0,\n",
    "            hidden_dropout=0.0,\n",
    "            feat_proj_dropout=0.0,\n",
    "            mask_time_prob=0.05,\n",
    "            layerdrop=0.0,\n",
    "            ctc_loss_reduction=\"mean\",\n",
    "            pad_token_id=self.processor.tokenizer.pad_token_id,\n",
    "            vocab_size=len(self.processor.tokenizer),\n",
    "        )\n",
    "        self.xlsr_model.freeze_feature_encoder()          # Freeze feature exctraction weights\n",
    "        self.xlsr_model.gradient_checkpointing_enable()   # Enable gradient checkpointing\n",
    "\n",
    "    def compute_metrics(self, pred):\n",
    "        pred_logits = pred.predictions\n",
    "        pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "        pred.label_ids[pred.label_ids == -100] = self.processor.tokenizer.pad_token_id\n",
    "        pred_str = self.processor.batch_decode(pred_ids)\n",
    "        # we do not want to group tokens when computing the metrics\n",
    "        label_str = self.processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "        wer_metric = evaluate.load(\"wer\")\n",
    "        wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "        return {\"wer\": wer}\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "            sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "            maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "            different lengths).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNUzS_ZV88_d"
   },
   "source": [
    "### 5. Log in to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "axOyNSKetkKY",
    "outputId": "0decef8f-4ee0-48c4-9e16-8be14378ea8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "login(token=WRITE_ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z78B15tc88_e"
   },
   "source": [
    "### 6. Instantiate ASR model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "4a3aecba59d8453ca799f737407dda8a",
      "3432af00d75643789d72270df3e4a767",
      "70ca129a01134938a52d4a742048fa41",
      "171283d85c7b450fa0e91042efd82b21",
      "95d603727b9146f0b5431b359dc84e06",
      "9713f7f9ead84b72a0fe6896d70ff834",
      "9ede668864f644c5968d668c0a5f0c10",
      "e0066d5275ad42d2af9f0c99d6fd9522",
      "37713092f67941b895c82295357ca373",
      "581437cc0e98454ab3944e9901f2b4e3",
      "bf8deebb10ec4423a0dc2c225bb64373",
      "b8cc7143cc024b34a1c84fb99d386aa5",
      "abb008cbc5ab435db0a75bd1f1c1bf9f",
      "fe8d4cf07645410c9d73f5d8d6bd0b83",
      "ca7b611400dc40f8bcf38898967180f0",
      "7481022234ba4cd2b46ac55f91f31852",
      "b8a22fd3d83d41c1bd3c6d1b9e90d761",
      "f5d50f3a4bfc4f7ab5e91639694e9820",
      "5b4766da67c949179e396b10cf8a4ba3",
      "587641afd9b640188bf28cb84c95dc4a",
      "0c9c4f870b1d481585db882a119e68d4",
      "4de2eafe374a4e72a25cb9a814ee53cb"
     ]
    },
    "id": "VugW1DTb88_e",
    "outputId": "13f1c804-27a1-4e5d-91ad-84c0260ff36f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3aecba59d8453ca799f737407dda8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/2.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8cc7143cc024b34a1c84fb99d386aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if dataset_name == \"fleurs_zu\":\n",
    "    model = ASR_MODEL(repo_name=PUSH_REPO,\n",
    "                      dataset_name=dataset_name,\n",
    "                      load_from_hf=False,\n",
    "                      push_dataset=False,\n",
    "                      push_repo=True,\n",
    "                      write_audio=True)\n",
    "else:\n",
    "    model = ASR_MODEL(repo_name=PUSH_REPO,\n",
    "                      dataset_name=dataset_name,\n",
    "                      load_from_hf=True,\n",
    "                      push_dataset=False,\n",
    "                      push_repo=True,\n",
    "                      write_audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe85WKiI88_f"
   },
   "source": [
    "### 7. Prepare ASR model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jNzgEMwhY0M"
   },
   "outputs": [],
   "source": [
    "if repo_exists:\n",
    "    !rm -rf {PUSH_REPO}\n",
    "    repo = Repository(local_dir=PUSH_REPO, clone_from=PULL_REPO)\n",
    "    model.load_datasets()\n",
    "    model.xlsr_model = Wav2Vec2ForCTC.from_pretrained(PUSH_REPO)\n",
    "    model.processor = Wav2Vec2Processor.from_pretrained(PUSH_REPO)\n",
    "    model.tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(PUSH_REPO)\n",
    "    model.xlsr_model.freeze_feature_encoder()          # Freeze feature exctraction weights\n",
    "    model.xlsr_model.gradient_checkpointing_enable()   # Enable gradient checkpointing\n",
    "    model.extract_features()\n",
    "    model.create_data_collator()\n",
    "elif sequential:\n",
    "    model.load_datasets()\n",
    "    model.create_tokenizer()\n",
    "    model.create_processor()\n",
    "    model.extract_features()\n",
    "    model.create_data_collator()\n",
    "else:\n",
    "    model.load_datasets()\n",
    "    model.create_tokenizer()\n",
    "    model.create_processor()\n",
    "    model.extract_features()\n",
    "    model.create_data_collator()\n",
    "    model.download_xlsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vb3g6yfl88_f"
   },
   "source": [
    "### 8. Train ASR model\n",
    "\n",
    "Useful links:\n",
    " - [Training arguments](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJZJin2xnR4S"
   },
   "outputs": [],
   "source": [
    "clear_cache()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # Where to save the model\n",
    "    push_to_hub=True,\n",
    "    overwrite_output_dir=True,\n",
    "    output_dir=model.repo_name,\n",
    "\n",
    "    # Batch sizes and num accumulation steps\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=3,\n",
    "\n",
    "    # Enable gradient checkpointing and half-point precision\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "\n",
    "    # Evaluation stuff\n",
    "    eval_steps=250,\n",
    "    evaluation_strategy=\"steps\",\n",
    "\n",
    "    # Saving stuff\n",
    "    save_steps=250,\n",
    "    save_total_limit=2,\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "\n",
    "    # Other hyperparameters\n",
    "    warmup_steps=500,\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=30,\n",
    "    group_by_length=True,\n",
    ")\n",
    "\n",
    "model.trainer = Trainer(\n",
    "    model=model.xlsr_model,\n",
    "    data_collator=model.data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=model.compute_metrics,\n",
    "    train_dataset=model.train_set,\n",
    "    eval_dataset=model.val_set,\n",
    "    tokenizer=model.processor.feature_extractor,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "model.trainer.train()\n",
    "model.trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c9c4f870b1d481585db882a119e68d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "171283d85c7b450fa0e91042efd82b21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_581437cc0e98454ab3944e9901f2b4e3",
      "placeholder": "​",
      "style": "IPY_MODEL_bf8deebb10ec4423a0dc2c225bb64373",
      "value": " 2.08k/2.08k [00:00&lt;00:00, 47.5kB/s]"
     }
    },
    "3432af00d75643789d72270df3e4a767": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9713f7f9ead84b72a0fe6896d70ff834",
      "placeholder": "​",
      "style": "IPY_MODEL_9ede668864f644c5968d668c0a5f0c10",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "37713092f67941b895c82295357ca373": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4a3aecba59d8453ca799f737407dda8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3432af00d75643789d72270df3e4a767",
       "IPY_MODEL_70ca129a01134938a52d4a742048fa41",
       "IPY_MODEL_171283d85c7b450fa0e91042efd82b21"
      ],
      "layout": "IPY_MODEL_95d603727b9146f0b5431b359dc84e06"
     }
    },
    "4de2eafe374a4e72a25cb9a814ee53cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "581437cc0e98454ab3944e9901f2b4e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "587641afd9b640188bf28cb84c95dc4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5b4766da67c949179e396b10cf8a4ba3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70ca129a01134938a52d4a742048fa41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e0066d5275ad42d2af9f0c99d6fd9522",
      "max": 2080,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_37713092f67941b895c82295357ca373",
      "value": 2080
     }
    },
    "7481022234ba4cd2b46ac55f91f31852": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95d603727b9146f0b5431b359dc84e06": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9713f7f9ead84b72a0fe6896d70ff834": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ede668864f644c5968d668c0a5f0c10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "abb008cbc5ab435db0a75bd1f1c1bf9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b8a22fd3d83d41c1bd3c6d1b9e90d761",
      "placeholder": "​",
      "style": "IPY_MODEL_f5d50f3a4bfc4f7ab5e91639694e9820",
      "value": "Downloading pytorch_model.bin:  94%"
     }
    },
    "b8a22fd3d83d41c1bd3c6d1b9e90d761": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b8cc7143cc024b34a1c84fb99d386aa5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_abb008cbc5ab435db0a75bd1f1c1bf9f",
       "IPY_MODEL_fe8d4cf07645410c9d73f5d8d6bd0b83",
       "IPY_MODEL_ca7b611400dc40f8bcf38898967180f0"
      ],
      "layout": "IPY_MODEL_7481022234ba4cd2b46ac55f91f31852"
     }
    },
    "bf8deebb10ec4423a0dc2c225bb64373": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca7b611400dc40f8bcf38898967180f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c9c4f870b1d481585db882a119e68d4",
      "placeholder": "​",
      "style": "IPY_MODEL_4de2eafe374a4e72a25cb9a814ee53cb",
      "value": " 1.18G/1.26G [00:37&lt;00:02, 30.8MB/s]"
     }
    },
    "e0066d5275ad42d2af9f0c99d6fd9522": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5d50f3a4bfc4f7ab5e91639694e9820": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fe8d4cf07645410c9d73f5d8d6bd0b83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b4766da67c949179e396b10cf8a4ba3",
      "max": 1262151917,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_587641afd9b640188bf28cb84c95dc4a",
      "value": 1184890880
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
